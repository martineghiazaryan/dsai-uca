{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:51.644352Z",
     "start_time": "2023-01-25T09:13:51.641752Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = np.array([\"I like chocolate\",\n",
    "            \"I like tea\",\n",
    "            \"You like chocolate\",\n",
    "            'You hate beer',\n",
    "            'I hate wine'])\n",
    "labels = np.array([1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:56.566199Z",
     "start_time": "2023-01-25T09:13:51.721495Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, TextVectorization, Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:56.569940Z",
     "start_time": "2023-01-25T09:13:56.567842Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW representation (the fist week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:57.293613Z",
     "start_time": "2023-01-25T09:13:56.572482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 17)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                576       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 609\n",
      "Trainable params: 609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "# with Keras preprocessing layer\n",
    "vectorize_layer = TextVectorization(output_mode='count', ngrams=(1,2))\n",
    "# Fit the layer with the corpus\n",
    "vectorize_layer.adapt(texts)\n",
    "\n",
    "# define the model\n",
    "input_ = Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize_layer(input_)\n",
    "hidden = Dense(32, activation='relu')(x)\n",
    "output_ = Dense(1, activation='sigmoid')(hidden)\n",
    "model = Model(input_, output_)\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:57.944180Z",
     "start_time": "2023-01-25T09:13:57.296277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 5)                0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " Embedding (Embedding)       (None, 5, 8)              80        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1312      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,425\n",
      "Trainable params: 1,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Accuracy: 60.000002\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "vocab_size = 10  # Maximum vocab size.\n",
    "max_len = 5      # Sequence length to pad the outputs to.\n",
    "embedding_size = 8\n",
    "\n",
    "# with Keras preprocessing layer\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_len)\n",
    "# Fit the layer with the corpus\n",
    "vectorize_layer.adapt(texts)\n",
    "\n",
    "# define the model\n",
    "input_ = Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize_layer(input_)\n",
    "x = Embedding(vocab_size, embedding_size, name=\"Embedding\")(x)\n",
    "x = Flatten()(x)\n",
    "hidden = Dense(32, activation=\"relu\")(x)\n",
    "output_ = Dense(1, activation='sigmoid')(hidden)\n",
    "model = Model(input_, output_)\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-trained embedding : Glove/Word2Vec/FastText embedding\n",
    "\n",
    "**Traditional word embedding** techniques (Glove/Word2Vec/FastText) learn a global word embedding. They first build a global vocabulary using unique words in the documents by ignoring the meaning of words in different context. Then, similar representations are learnt for the words appeared more frequently close each other in the documents. The problem is that in such word representations the words' contextual meaning (the meaning derived from the words' surroundings), is ignored. For example, only one representation is learnt for \"left\" in sentence \"I left my phone on the left side of the table.\" However, \"left\" has two different meanings in the sentence, and needs to have two different representations in the embedding space.\n",
    "\n",
    "For example, consider the two sentences:\n",
    "\n",
    "1. I will show you a valid point of reference and talk to the point.\n",
    "1. Where have you placed the point.\n",
    "\n",
    "The word embeddings from a pre-trained embeddings such as word2vec, the embeddings for the word 'point' is same for both of its occurrences in example 1 and also the same for the word 'point' in example 2. (all three occurrences has same embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:58.006266Z",
     "start_time": "2023-01-25T09:13:57.946147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same steps as Keras Embedding\n",
    "vocab_size = 10  # Maximum vocab size.\n",
    "max_len = 5      # Sequence length to pad the outputs to.\n",
    "hidden_size = 16\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len)\n",
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:58.013870Z",
     "start_time": "2023-01-25T09:13:58.008088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'like': 2,\n",
       " 'i': 3,\n",
       " 'you': 4,\n",
       " 'hate': 5,\n",
       " 'chocolate': 6,\n",
       " 'wine': 7,\n",
       " 'tea': 8,\n",
       " 'beer': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build word dict\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:13:58.017463Z",
     "start_time": "2023-01-25T09:13:58.015494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the pre-trained embedding matrix for exemple from glove\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:01.788812Z",
     "start_time": "2023-01-25T09:13:58.021161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Make a dict mapping words (strings) to their NumPy vector representation:\n",
    "path_to_glove_file = \"/users/riveill/DS-models/glove.6B.50d.txt\"\n",
    "\n",
    "# pre-trained embedding matrix\n",
    "embedding_dim = 50 # fixed by thepre-trained embedding matrix\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:01.796131Z",
     "start_time": "2023-01-25T09:14:01.791677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "[UNK] 1\n",
      "like 2\n",
      "i 3\n",
      "you 4\n",
      "hate 5\n",
      "chocolate 6\n",
      "wine 7\n",
      "tea 8\n",
      "beer 9\n",
      "Converted 8 words (2 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2 # UNK/OOV and PAD\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    print(word, i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:01.802967Z",
     "start_time": "2023-01-25T09:14:01.798107Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 50, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Embedding layer with the weight of each word\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "num_tokens,embedding_dim, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:02.352277Z",
     "start_time": "2023-01-25T09:14:01.804338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 5)                0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 5, 50)             600       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                8032      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,665\n",
      "Trainable params: 8,065\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize_layer(input_)\n",
    "x = embedding_layer(x)\n",
    "x = Flatten()(x)\n",
    "hidden = Dense(32, activation=\"relu\")(x)\n",
    "output_ = Dense(1, activation='sigmoid')(hidden)\n",
    "model = Model(input_, output_)\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your own Word2Vec model with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:02.356962Z",
     "start_time": "2023-01-25T09:14:02.354619Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, you need data to train a model. We will use part of the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:04.380207Z",
     "start_time": "2023-01-25T09:14:02.358684Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_set = brown.sents()[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train a model on our corpus. Don't worry about the training parameters much for now, we'll revisit them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.579790Z",
     "start_time": "2023-01-25T09:14:04.382471Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=train_set, size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "#model = Word2Vec(sentences=common_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model, we can use it.\n",
    "\n",
    "The main part of the model is model.wv\\ , where \"wv\" stands for \"word vectors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.586511Z",
     "start_time": "2023-01-25T09:14:10.581863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04136918, -0.10978555, -0.15034047, -0.24463718, -0.05027288,\n",
       "       -0.07669671,  0.06218094,  0.00680144, -0.08232249, -0.33849627,\n",
       "       -0.2387926 ,  0.02191992, -0.06522922,  0.33818543,  0.18337403,\n",
       "        0.10902342,  0.03109928, -0.07143142, -0.12798679,  0.01539179,\n",
       "       -0.03205783,  0.04300994,  0.12229457, -0.29632238,  0.05040243,\n",
       "        0.00419194,  0.09828603,  0.06172842, -0.23809716, -0.10589421,\n",
       "        0.02364612, -0.11272949, -0.03686354,  0.18639238,  0.18105426,\n",
       "       -0.31819963, -0.41908318,  0.18029644,  0.2593548 ,  0.19554946,\n",
       "        0.22721523, -0.15141046,  0.17618845, -0.16063029,  0.10343505,\n",
       "       -0.0245653 , -0.16474013, -0.16749346,  0.18614413,  0.3696545 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['university']  # get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.592740Z",
     "start_time": "2023-01-25T09:14:10.588512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99821913"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.610165Z",
     "start_time": "2023-01-25T09:14:10.594936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('series', 0.9993743896484375),\n",
       " ('meeting', 0.9993742108345032),\n",
       " ('program', 0.9993041753768921),\n",
       " ('came', 0.99930340051651),\n",
       " ('other', 0.999301016330719),\n",
       " ('court', 0.999296247959137),\n",
       " ('early', 0.9992777109146118),\n",
       " ('history', 0.9992622137069702),\n",
       " ('hand', 0.9992523193359375),\n",
       " ('two', 0.999245285987854)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar('university', topn=10)  # get other similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T14:04:08.927613Z",
     "start_time": "2022-01-21T14:04:08.890931Z"
    }
   },
   "source": [
    "Training non-trivial models can take time.  Once the model is built, it can be saved using standard gensim methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.866751Z",
     "start_time": "2023-01-25T09:14:10.612442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/3c9gtfld201dy53fjq35ky7c0000gn/T/gensim-model-nmy5oces\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    print(temporary_filepath)\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save the model you can continue training it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.880240Z",
     "start_time": "2023-01-25T09:14:10.868501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "new_model.train([word_tokenize(sent) for sent in texts], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you no longer need to retrain the model, it can be saved with only the vectors and their keys. This results in a much smaller and faster object that can be loaded more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.991257Z",
     "start_time": "2023-01-25T09:14:10.881805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04136918, -0.10978555, -0.15034047, -0.24463718, -0.05027288,\n",
       "       -0.07669671,  0.06218094,  0.00680144, -0.08232249, -0.33849627,\n",
       "       -0.2387926 ,  0.02191992, -0.06522922,  0.33818543,  0.18337403,\n",
       "        0.10902342,  0.03109928, -0.07143142, -0.12798679,  0.01539179,\n",
       "       -0.03205783,  0.04300994,  0.12229457, -0.29632238,  0.05040243,\n",
       "        0.00419194,  0.09828603,  0.06172842, -0.23809716, -0.10589421,\n",
       "        0.02364612, -0.11272949, -0.03686354,  0.18639238,  0.18105426,\n",
       "       -0.31819963, -0.41908318,  0.18029644,  0.2593548 ,  0.19554946,\n",
       "        0.22721523, -0.15141046,  0.17618845, -0.16063029,  0.10343505,\n",
       "       -0.0245653 , -0.16474013, -0.16749346,  0.18614413,  0.3696545 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Store just the words + their trained embeddings.\n",
    "word_vectors = new_model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "\n",
    "# Load back with memory-mapping = read-only, shared across processes.\n",
    "new_word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "\n",
    "vector = new_word_vectors['university']  # Get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the template exactly as if it were a Glove/Word2Vec/FastText template retrieved from the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:10.997724Z",
     "start_time": "2023-01-25T09:14:10.992845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'like': 2,\n",
       " 'i': 3,\n",
       " 'you': 4,\n",
       " 'hate': 5,\n",
       " 'chocolate': 6,\n",
       " 'wine': 7,\n",
       " 'tea': 8,\n",
       " 'beer': 9}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build word dict\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:11.002974Z",
     "start_time": "2023-01-25T09:14:10.999368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 words (3 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = new_word_vectors[word]\n",
    "        hits += 1\n",
    "    except :\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:11.011460Z",
     "start_time": "2023-01-25T09:14:11.008110Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    name=\"Embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:11.577234Z",
     "start_time": "2023-01-25T09:14:11.013833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 5)                0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " Embedding (Embedding)       (None, 5, 50)             600       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 250)               0         \n",
      "                                                                 \n",
      " Hidden (Dense)              (None, 32)                8032      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,665\n",
      "Trainable params: 8,065\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_ = Input(shape=(1,), dtype=tf.string, name=\"Input\")\n",
    "x = vectorize_layer(input_)\n",
    "x = embedding_layer(x)\n",
    "x = Flatten()(x)\n",
    "hidden = Dense(32, activation=\"relu\", name=\"Hidden\")(x)\n",
    "output_ = Dense(1, activation='sigmoid', name=\"Output\")(hidden)\n",
    "model = Model(input_, output_)\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a all the pre-trained embedding : Glove/Word2Vec/FastText embedding (this week)\n",
    "\n",
    "Up to now, the embedding is a matrix of size vocab_size * embedding_size\n",
    "* vocab_size being the number of tokens in the training data: for example in the previous situation, this size was fixed at 5000\n",
    "* The objective here is to have a matrix of size pre_traine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:15.416894Z",
     "start_time": "2023-01-25T09:14:11.579332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary list\n",
    "# Build the embedding matrix\n",
    "path_to_glove_file = \"/users/riveill/DS-models/glove.6B.50d.txt\"\n",
    "\n",
    "vocabulary = []\n",
    "embedding_matrix = [np.zeros((embedding_dim)),\n",
    "                    np.zeros((embedding_dim))] # See later : 0=PAD, 1=OOV\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        vocabulary += [word]\n",
    "        embedding_matrix += [coefs]\n",
    "embedding_matrix = np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:15.422399Z",
     "start_time": "2023-01-25T09:14:15.418657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:15.427870Z",
     "start_time": "2023-01-25T09:14:15.424304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400002)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(embedding_matrix)\n",
    "len(vocabulary), len(embedding_matrix), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:18.393063Z",
     "start_time": "2023-01-25T09:14:15.429589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vectorizer layer and initialize it with the vocabulary list\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=len(embedding_matrix),\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_len,\n",
    "        vocabulary=vocabulary  # Pass the vocabulary - no need to adapt the layer\n",
    "                               # Contain the padding token ('') and OOV token ('[UNK]')\n",
    ")\n",
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:19.303668Z",
     "start_time": "2023-01-25T09:14:18.395263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', ',', '.', 'of', 'to', 'and', 'in', 'a']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the begining of the vocabulary list\n",
    "vectorize_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:19.645232Z",
     "start_time": "2023-01-25T09:14:19.305957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([     2, 400001,      1,      0,      0])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test vectorizer layer\n",
    "vectorize_layer('the sandberger oov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:19.649232Z",
     "start_time": "2023-01-25T09:14:19.647255Z"
    }
   },
   "outputs": [],
   "source": [
    "# The rest is similar to an approach with Keras embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:19.655079Z",
     "start_time": "2023-01-25T09:14:19.651542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False, # False: don't fine tune the embedding matrix / True: fine tune\n",
    "    name=\"Embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T09:14:20.149052Z",
     "start_time": "2023-01-25T09:14:19.657430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7fafab94ec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(texts, labels, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(texts, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}