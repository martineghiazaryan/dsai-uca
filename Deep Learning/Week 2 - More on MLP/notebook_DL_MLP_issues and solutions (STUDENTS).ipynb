{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_c9ULzNvq_g"
   },
   "source": [
    "# Deep Learning: common issues and solutions\n",
    "\n",
    "This notebook presenting various techniques must be handed in. It will be marked.\n",
    "You must add your own comments and tests. It is the comments and your own tests that will be assessed.\n",
    "* Commentary when comparing different approaches\n",
    "* Own test, when testing different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner --upgrade\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some global constant\n",
    "epochs=100\n",
    "batch_size=256\n",
    "patience=10\n",
    "hidden_dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usual function for babysit the network\n",
    "\n",
    "# It is important to systematically observe the learning curves\n",
    "def babysit(history):\n",
    "    keys = [key for key in history.keys() if key[:4] != \"val_\"]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(keys), figsize=(18, 5))\n",
    "    for i, key in enumerate(keys):\n",
    "        ax[i].plot(history[key], label=key)\n",
    "        if \"val_\"+key in history.keys():\n",
    "            ax[i].plot(history[\"val_\"+key], label=\"val_\"+key)\n",
    "        ax[i].legend()\n",
    "        ax[i].set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usual callback for training deep learning model\n",
    "\n",
    "# It is important to use early stopping systematically\n",
    "callbacks_list = [EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                                patience=patience,\n",
    "                                restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgzggwPnvq_o"
   },
   "source": [
    "## 1. Today lab\n",
    "\n",
    "In this lab we use part of the 'Amazon_Unlocked_Mobile.csv' dataset published by Kaggle. The dataset contain the following information:\n",
    "* Product Name\n",
    "* Brand Name\n",
    "* Price\n",
    "* Rating\n",
    "* Reviews\n",
    "* Review Votes\n",
    "\n",
    "We are mainly interested by the 'Reviews' (X) and by the 'Rating' (y)\n",
    "\n",
    "As you did in the previous lab, the goal is to try to predict the 'Rating' after reading the 'Reviews'.\n",
    "We will mostly use this dataset as a case study to illustrate issues that you can have using Multilayer Perceptron or other Deep Learning architectures, namely:\n",
    "\n",
    "1) **Text preprocessing with Tensorflow API**\n",
    "\n",
    "2) **The vanishing gradient problem**:\n",
    "\n",
    "Problem: Your model does not learn at all !\n",
    "    \n",
    "3) **Underfitting and Overfitting problems**\n",
    "Problems:\n",
    "\n",
    "    - Underfitting relates to the fact that your model does not learn enough on the train dataset to hope for good generalization abilities (good label prediction on new samples with unknown labels).\n",
    "    - Overfitting means that your model fits too much to the train dataset, which can also prevents it from generalizing well to new samples with unknown labels.\n",
    "    \n",
    "4) **Starting, stopping, and resuming training**\n",
    "\n",
    "Learning how to start, stop and resume learning a deep learning model is a very important skill to master. At some point:\n",
    "\n",
    "* You have limited time on a GPU instance (this can happen on Google Colab or when using the cheaper Amazon EC2 point instances).\n",
    "* Your SSH connection is broken.\n",
    "* Your deep learning platform crashes and shuts down.\n",
    "\n",
    "Imagine you've spent a whole week training a state-of-the-art deep neural network... and your model is lost due to a power failure! \n",
    "\n",
    "5) Find best hyper-parameters with keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZg2dPXvvq_q"
   },
   "source": [
    "## 2. Dataset pre-processing\n",
    "\n",
    "In this lab, we will just re-use the dataset of previous lab providing Sentiment Analysis tasks. \n",
    "And we will stick to the tf-idf approach for word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vlbVhVpvq_s"
   },
   "source": [
    "### a) Essential reminder [About Train, validation and test sets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "![test/train/val](https://miro.medium.com/max/1466/1*aNPC1ifHN2WydKHyEZYENg.png)\n",
    "\n",
    "* **Training Dataset:** The sample of data used to fit the model.\n",
    "* **Validation Dataset:** The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "* **Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "**If you use cross validation, concatenate Train and Validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T16:26:01.438539Z",
     "start_time": "2021-10-20T16:26:00.299220Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2633,
     "status": "ok",
     "timestamp": 1666197805611,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "MLWzp9CAvq_t",
    "outputId": "38cd0023-f6ea-4a1a-f2da-39bbf9832b5e"
   },
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "TEST = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")\n",
    "\n",
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2eC1L4fvq_0"
   },
   "source": [
    "### b) Build X (features vectors) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:16:24.050760Z",
     "start_time": "2021-01-04T10:16:24.046179Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1666197813879,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "dOnscXEBvq_0",
    "outputId": "30b0f077-0907-4cdf-c76e-c0be57030a57"
   },
   "outputs": [],
   "source": [
    "# Construct X_train and y_train\n",
    "X_train = TRAIN['Reviews'].fillna(\"\")\n",
    "y_train = TRAIN['Rating']\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:16:26.409790Z",
     "start_time": "2021-01-04T10:16:26.405136Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1666197815404,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "338OMkmHvq_1",
    "outputId": "68ad0cff-9ccd-45b9-cfb1-fae11fea08ef"
   },
   "outputs": [],
   "source": [
    "# Construct X_test and y_test\n",
    "X_test = TEST['Reviews'].fillna(\"\")\n",
    "y_test = TEST['Rating']\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1666199574684,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "Z_XKcRR9vq_3"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "y_train_encoded = ohe.fit_transform(y_train)\n",
    "y_val_encoded = ohe.transform(y_val)\n",
    "y_test_encoded = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 866,
     "status": "ok",
     "timestamp": 1666199411424,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "gTndrlXFND8u"
   },
   "outputs": [],
   "source": [
    "# Define constant\n",
    "n_classes = len(np.unique(y_train))\n",
    "feature_vector_length = X_train.shape[1]\n",
    "\n",
    "feature_vector_length, n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text preprocessing with tensorflow\n",
    "\n",
    "So far we have used `sklearn.feature_extraction.text.CountVectorize` or `sklearn.feature_extraction.text.TfidfVectorize` preceded by our own preprocessing to transform a text sequence into a vector. Unfortunately it is not possible to integrate this into a 'tensorflow' pipeline or vice versa, integrating a Tensorflow network into a sklearn pipeline is not easy. \n",
    "\n",
    "Fortunately, Tensorflow has a similar function: [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)\n",
    "\n",
    "Look at the Tensorflow documentation to understand how it works.\n",
    "\n",
    "The main parameter is output_mode:\n",
    "* \"int\": Outputs integer indices, one integer index per split string token. When output_mode == \"int\", 0 is reserved for masked locations; this reduces the vocab size to max_tokens - 2 instead of max_tokens - 1.\n",
    "    * give an ID for each token\n",
    "    \n",
    "Below is a small example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I love chocolate and I hate beer\",\n",
    "          \"I love beer and I hate chocolate\",\n",
    "          \"I love beer and I love chocolate\"]\n",
    "corpus = tf.convert_to_tensor(corpus)\n",
    "\n",
    "for output_mode in ['multi_hot', 'count', 'tf_idf', 'int']:\n",
    "    print(\"-\"*50)\n",
    "    print(\"output_mode:\", output_mode)\n",
    "    vectorize_layer = layers.TextVectorization(output_mode=output_mode)\n",
    "    vectorize_layer.adapt(corpus) # Do the same thinks as fit in sklearn library\n",
    "    print(vectorize_layer.get_vocabulary())\n",
    "    print(vectorize_layer(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[TO DO STUDENTS]</font>\n",
    "\n",
    "It is up to you to build examples with the other parameters of the TextVectorization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[ TO DO STUDENTS]</font>\n",
    "\n",
    "Initialize your vectorizer layer according to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vanishing gradient problem\n",
    "\n",
    "This problem can be encountered when training NN with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural network's weights receives an update proportional to the partial derivative of the loss function with respect to the current weight. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. This mostly occurs when your architecture counts too many parameters to learn.\n",
    "\n",
    "Possible solutions, obviously with their pros and cons: \n",
    "\n",
    "* Reduce the depth of your network.\n",
    "* Use sparsity promoting activation functions such as the ReLU activation function, i.e ReLU(x)= max(0, x)\n",
    "* Use residual connections, i.e output at each layer: layer(input) + input\n",
    "* Use normalization techniques, e.g Batch Normalization and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Observe the vanishing gradient problem\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS]</font>\n",
    "\n",
    "Design a function to simply build MLP with the following inputs, which return the model ready to compile:\n",
    "\n",
    "* vectorizer: the vectorizer layer used to transform a sentence in a vector</font>\n",
    "* activation: activation used at each hidden layer\n",
    "* n_hiddenlayers: number of hidden layers in the network\n",
    "* hidden_dim: shared number of neurons within each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vectorizer, activation, n_hiddenlayers, hidden_dim):\n",
    "    \"\"\" Your code here \"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1666199411424,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "s74KgD5xNLIs",
    "outputId": "d848f269-7611-45d1-cbc7-4bb7c1a50100",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a network with 30 hidden layers with 'tanh' activations\n",
    "model = build_model(vectorizer, 'tanh', 30, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "plot_model(model, show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    #show_layer_names=True,\n",
    "    #rankdir=\"TB\",\n",
    "    #expand_nested=True,\n",
    "    dpi=64,\n",
    "    #layer_range=True,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">You now know 2 ways to view your network. You can choose the one you prefer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21474,
     "status": "ok",
     "timestamp": 1666199432891,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "TtmjaXNWNLIt",
    "outputId": "09962ab2-38d7-45ff-adb4-376b3bed342b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training, use the defined early stopping\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1666199556038,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "oG9YbWLqNLIu",
    "outputId": "880ccd52-785b-412f-9da4-e7452c066eee"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[ TO DO STUDENTS]</font>\n",
    "\n",
    "Is your network learning? Check your intuition by evaluating your model and looking at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print/plot the confusion matrix\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Experiment on ReLU activation\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS]</font>\n",
    "\n",
    "Change activation from 'tanh' to 'relu', still with a deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666199558281,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "Oqspf948OA1D",
    "outputId": "74953e77-987e-45d2-9074-eeb9f2242a17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a network with 30 hidden layers with 'tanh' activations\n",
    "model = build_model(vectorizer, 'relu', 30, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666199560857,
     "user": {
      "displayName": "Antoine Collin",
      "userId": "01107865217976062482"
     },
     "user_tz": -120
    },
    "id": "4RHvQS6qN228",
    "outputId": "601bd34e-1c9f-43df-d165-72fa096da486"
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[ TO DO STUDENTS]</font>\n",
    "\n",
    "Does the network learn better? Does the network perform well? Study the learning curves and justify your statements with the study of its performance (classification report and confusion matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my code, it seems that the ReLU activation for sparsity has helped to solve the problem, but the model still struggles to learn and to get good performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Experiment on residual connections\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS ]<color>\n",
    "* Create a function to generate models with residual connections.\n",
    "* Using ReLU activation + residual connections, are you able to get better results ?\n",
    "* Provide here a description of the learning and performances of your network\n",
    "* Compare it to previous models. What are your conclusions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_residual_model(vectorizer, activation, n_hiddenlayers, hidden_dim):\n",
    "    \"\"\" your code here \"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a network with 30 hidden layers with 'relu' activations\n",
    "model = build_residual_model(vectorizer, 'relu', 30, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and plot the model --> What is the best solution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Experiment on Batch Normalization\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS ]</font>\n",
    "* step1: adapt build_model function to add batch normalization layers after the output of the hidden dense layers. \n",
    "* step2: adapt build_model function to use batch normalization layers and residuals\n",
    "             \n",
    "* In both case use ReLU activation and 30 hidden layers as previouly\n",
    "* Compare your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_batch_normalization(vectorizer, activation, n_hiddenlayers, hidden_dim):\n",
    "    \"\"\" Your code here \"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_residual_model_residual_batch_normalization(vectorizer, activation, n_hiddenlayers, hidden_dim):\n",
    "    \"\"\" your code here \"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train the network with BatchNormalization layer\n",
    "model = build_model_batch_normalization(vectorizer, 'relu', 30, hidden_dim)\n",
    "\n",
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with `build_residual_model_residual_batch_normalization`\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) What if you simply reduce the network depth ?\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS ]</font>\n",
    "* build a MLP with ReLU activation composed of 10 hidden layers\n",
    "* compare your results both in terms of learning and performances compared to other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build and train the network without residual connections\n",
    "model = build_model(vectorizer, 'relu', 10, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you observed a typical instance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Underfitting and Overfitting problems\n",
    "\n",
    "Actually what you observed in the last experiment is a typical instance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Decrease the network size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train the network without residual connections\n",
    "model = build_model(vectorizer, 'relu', 5, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me: it gets slightly better but almost the same behavior is observed when taking 10 and 5 hidden layers.\n",
    "\n",
    "What's going on for you ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Experiment on L2 regularization\n",
    "\n",
    "<font color=\"red\">[ TO DO STUDENTS ]</font>\n",
    "* check the keras documentation on regularizations https://keras.io/api/layers/regularizers/\n",
    "* Add to the previous network L2 regularization: first with l2_reg = 0.01 / then with l2_reg= 0.0001\n",
    "* Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the model function\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_model_reg(vectorizer, activation, n_hiddenlayers, hidden_dim, l2_reg = 0.01):\n",
    "    \"\"\" your code here \"\"\"\n",
    "    \"\"\" your code use: kernel_regularizer and bias_regularizer parameters of Dense\"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train the network without residual connections\n",
    "model = build_model_reg(vectorizer, 'relu', 5, hidden_dim, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[ TO DO STUDENTS]</font code=\"red\">\n",
    "\n",
    "Reduce the coefficient of L2 regularization taken into account in the loss from l2_reg = 0.01 > to l2_reg = 0.0001 and do the same experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your conclusion ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Experiment on Dropout\n",
    "\n",
    "<font color=\"red\">[TO DO STUDENTS]</font>\n",
    "* Observe the provided results for a dropout ratio of p=0.7 and p=0.3\n",
    "* What are your conclusions ?\n",
    "* In the end, considering all the explored settings in this Lab, what would you suggest as a network to get a better model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the model function\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model_dropout(vectorizer, activation, n_hiddenlayers, hidden_dim, p = 0.5):\n",
    "    \"\"\" your code here \"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the network without residual connections\n",
    "model = build_model_dropout(vectorizer, 'relu', 5, hidden_dim, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves and analyze them\n",
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">[ TO DO STUDENTS]</font code=\"red\">\n",
    "\n",
    "Decrease the proportion of neurons deactivated at each forward pass, from 0.7 to 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" your code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your conclusion ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and resume training\n",
    "\n",
    "Learning how to start, stop and resume learning a deep learning model is a very important skill to master. At some point:\n",
    "\n",
    "* You have limited time on a GPU instance (this can happen on Google Colab or when using the cheaper Amazon EC2 point instances).\n",
    "* Your SSH connection is broken.\n",
    "* Your deep learning platform crashes and shuts down.\n",
    "\n",
    "Imagine you've spent a whole week training a state-of-the-art deep neural network... and your model is lost due to a power failure! Fortunately, there is a solution - but when these situations occur, you need to know what to do:\n",
    "\n",
    "1. Take a snapshot model that was saved/serialized to disk during training.\n",
    "1. Load the model into memory.\n",
    "1. Resume training where you left off.\n",
    "\n",
    "Starting, stopping and resuming training is standard practice when setting the learning rate manually:\n",
    "\n",
    "1. Start training your model until the loss/accuracy reaches a plateau.\n",
    "1. Take a snapshot of your model every N epochs (typically N={1, 5, 10})\n",
    "1. Stop training when you arrive at a plateau (by forcing out via ctrl + c or via earlystopping\n",
    "1. Adjust your learning rate (typically by reducing it by an order of magnitude).\n",
    "1. Restart the training script, starting from the last snapshot of the model weights\n",
    "\n",
    "The ability to adjust the learning rate is an essential skill for any deep learning practitioner to master, so take the time to study and practice it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Look at the documentation Tensorflow has proposed for ModelCheckpoint: we want to save the model at the end of each epoch so that we can restore it later.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse one the previous model and reset it\n",
    "# Use sgd as optimizer and fix learning_rate = 0.1\n",
    "# Use 2 callbacks : EarlyStopping and ModelCheckpoint\n",
    "# Save model at each epoch\n",
    "# Fit the network\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = build_model_dropout(vectorizer, 'relu', 5, hidden_dim, 0.5)\n",
    "\n",
    "opt = optimizers.SGD(learning_rate=0.1) # Fix learning rate to 0.1\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                                patience=patience,\n",
    "                                restore_best_weights=True),\n",
    "                  ModelCheckpoint(filepath=...,\n",
    "                                  # Complete, if necessary\n",
    "                                 )]\n",
    "history1 = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded),\n",
    "                    epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model\n",
    "new_model = tf.keras.models.load_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between load_model and load_weights ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change learning_rate = 0.01\n",
    "new_model.optimizer.lr.assign(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to fit the network\n",
    "history2 = new_model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded),\n",
    "                    epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras-tuner\n",
    "\n",
    "<font color=\"red\">[TO DO STUDENTS]</font>\n",
    "\n",
    "From the previous experiences use Keras-tuner to find the best possible network.\n",
    "\n",
    "Keras-Tuner must build at least 3 different network architectures:\n",
    "* Dense cells only\n",
    "* Addition of residuals\n",
    "* Adding batch normalisation\n",
    "* Adding dropout\n",
    "* A combination of the different additions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" your code here \"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
