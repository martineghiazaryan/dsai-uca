{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d04eef",
   "metadata": {},
   "source": [
    "# Lab 1: Gradient descent\n",
    "_Samuel Vaiter_ (<samuel.vaiter@cnrs.fr>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa37198",
   "metadata": {},
   "source": [
    "In this practical, we are going to study the **gradient descent** algorithm, and several variations of it, through the lenses of three examples:\n",
    "1. Two simple 2D functions to optimize, easy to visualize thanks to `matplotlib.pyplot.contour`\n",
    "2. The ordinary least square problem\n",
    "3. Digits classification with multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e06d27",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "1. You need to send me your completed jupyter notebook at <samuel.vaiter@cnrs.fr> **one week** (before start of class time + 168h) after the end of the session with subject \n",
    "    \n",
    "    `[Opt Lab 1/2023] LASTNAME Firstname`\n",
    "    \n",
    "2. All the Python cells should be error-free.\n",
    "3. When a theoretical question is asked in a cell, fill your answer in Markdown/LaTeX in it. You should give the mathematical steps necessary to reproduce your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bd7d6",
   "metadata": {},
   "source": [
    "## Toy example in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d3d23",
   "metadata": {},
   "source": [
    "We are going to test first our methods on two functions, one convex (potentially strongly convex), the other not.\n",
    "\n",
    "The convex function is going to be a simple quadratic form on $\\mathbb{R}^2$ defined by\n",
    "$$ f(x) = (x_0 - x_1 - 1)^2 + \\frac{\\mu}{2} x_0^2 , $$\n",
    "for some $\\mu \\in \\mathbb{R}$.\n",
    "\n",
    "**Q1**. Does the function $f$ is convex? When is it strongly convex?\n",
    "\n",
    "**Q2**. Determine its unique minimizer when $\\mu > 0$.\n",
    "\n",
    "**Q3**. Implement it as a function `f2D_convex(x, mu=1.0)` taking as arguments the 2D vector `x` and the strong-convexity parameter `mu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41623c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2D_convex(x, mu=1.0):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa85540",
   "metadata": {},
   "source": [
    "**Q4**. Display the levelsets of the function `f2D_convex` around $(0,0)$. Modify the value of $\\mu$ and see how the geometry change. _Hint_: You should look at the help of `np.meshgrid`, `plt.contourf` and eventually `np.vectorize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ef3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the level sets of `f2D_convex`\n",
    "x = np.linspace(-2,2,50)\n",
    "y = np.linspace(-2,2,50)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = np.vectorize(lambda x,y: f2D_convex(np.array([x,y])))(X,Y)\n",
    "plt.contourf(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472ae23",
   "metadata": {},
   "source": [
    "**Q5**. Observe what happens when $\\mu = 0$. Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81357b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5313be4",
   "metadata": {},
   "source": [
    "We now turn to another function, \n",
    "$$ f(x) = \\cos(x_0) + \\sin(x_1) . $$\n",
    "\n",
    "**Q6**. Is it a convex function? Describe the minima of $f$.\n",
    "\n",
    "**Q7**. Implement it as a function `f2D_nonconvex(x)` taking as arguments the 2D vector `x`. Display its levelsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4664bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2D_nonconvex(x):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e844d",
   "metadata": {},
   "source": [
    "## Gradient descent procedure\n",
    "\n",
    "We recall that a gradient step takes the form\n",
    "$$ x^{(t+1)} = x^{(t)} - \\eta^{(t)} \\nabla f(x^{(t)}) $$\n",
    "with some initialization $x^{(0)}$ and $\\eta^{(t)}$ a learning rate policy.\n",
    "\n",
    "**Q1**. Implement a function `gradient_descent_tmp` taking as argument `grad_f`, `eta`, `x0` and `max_iter`. `grad_f` will take as input an optimization variable `x`, `eta` is the learning policy taking as arguments `x` and `t` the current iteration, `x0` is the initialization and `max_iter` a maximal number of iterations. As an example, a constant learning rate policy can defined as\n",
    "```python\n",
    "def constant_eta(x, t):\n",
    "    return 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_tmp(grad_f, eta, x0, max_iter):\n",
    "    x = x0\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8db654",
   "metadata": {},
   "source": [
    "**Q2**. Implement a function `grad_f2D_convex` returning the gradient of the convex function defined in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f2D_convex(x, mu=1.0):\n",
    "    return np.array([\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412fbb3",
   "metadata": {},
   "source": [
    "**Q3**. Compute the Lipschitz constant of $\\nabla f(x)$ and deduce a learning rate policy `lr_policy_f2D-convex(x,t)`. Discuss the difference between $\\mu = 0$ and $\\mu \\neq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a233f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_f2D_convex(x, mu=1.0):\n",
    "    return np.array([\n",
    "        [2+mu, -2],\n",
    "        [-2, 2]\n",
    "    ])\n",
    "\n",
    "\n",
    "def lr_policy_f2D_convex(x, t):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8e23d",
   "metadata": {},
   "source": [
    "**Q4**. Test these functions, and observe the impact of `max_iter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_tmp(grad_f2D_convex, lr_policy_f2D_convex, np.zeros(2), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18ff9d",
   "metadata": {},
   "source": [
    "**Q5**. Implement a function `gradient_descent` taking as arguments `grad_f`, `eta`, `x0`, `max_iter` (defined as in *2.Q1*), `return_all` a boolean indicating if we keep in memory all iterates and `callback` a callable that is called at each iteration if not `None`. If `return_all` and `callback` are `None`, it should behave like `gradient_descent_tmp`. Otherwise, it should return a `dict` containing the final iterate, and eventually all the iterates and outputs of the callback. The function may look like the following stub\n",
    "```python\n",
    "def gradient_descent(grad_f, eta, x0, max_iter, return_all=False, callback=None):\n",
    "    x = x0\n",
    "    fxs = []\n",
    "    xs = []\n",
    "    # Do something\n",
    "    if not callback and not return_all:\n",
    "        return x\n",
    "    else:\n",
    "        return {\n",
    "            'res': x,\n",
    "            'callbacks': fxs,\n",
    "            'iterates': xs,\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ab301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grad_f, eta, x0, max_iter, return_all=False, callback=None):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333a059",
   "metadata": {},
   "source": [
    "**Q6**. Display the gradient descent on `f2D_convex` by plotting the function value both in linear scale, and loglog scale. Look at the documentation of `matplotlib.pyplot.loglog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48173dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gradient_descent(grad_f2D_convex, lr_policy_f2D_convex, np.zeros(2), 100, return_all=True, callback=f2D_convex)\n",
    "fig, ax = plt.subplots(2,1)\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "ax[0].set_xlabel('iterate $t$')\n",
    "ax[0].set_ylabel('$f(x^{(t)})$')\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "ax[1].set_xlabel('iterate $\\log t$')\n",
    "ax[1].set_ylabel('$\\log f(x^{(t)})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b1848",
   "metadata": {},
   "source": [
    "**Q7**. Visualize the gradient steps on a contour plot. Try several initializations. Try several step-sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e788d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,50)\n",
    "y = np.linspace(-1.5,0.5,50)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = np.vectorize(lambda x,y: f2D_convex(np.array([x,y])))(X,Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "ax.contourf(X, Y, Z, 20)\n",
    "ax.contour(X,Y,Z, 20, colors='k')\n",
    "\n",
    "# Display the iterates\n",
    "xs = res['iterates']\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "sol = np.array([0,-1])\n",
    "ax.plot(sol[0],sol[1],marker='x', color ='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e1e72",
   "metadata": {},
   "source": [
    "## Let's go to higher dimension.\n",
    "\n",
    "For a matrix $A \\in \\mathbb{R}^{n \\times p}$ and a vector $b \\in \\mathbb{R}^{n}$, the least-square objective reads\n",
    "$$ f(x) = \\frac{1}{2} \\| A x - b \\|_2^2 . $$\n",
    "\n",
    "**Q1**. Generate a matrix `A` of size 100 by 200 with i.i.d. normal entries and `b` also i.i.d. normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b127465",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 200\n",
    "rng = np.random.default_rng()\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e11a1",
   "metadata": {},
   "source": [
    "**Q2**. Implement `least_square` and its gradient `grad_least_square` with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(x):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "\n",
    "def grad_least_square(x):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9d3c1",
   "metadata": {},
   "source": [
    "**Q3**. Compute the Lipschitz constant $L$ of $\\nabla f$ and write a learning rate policy `lr_policy_least_square` _independant_ from the strong-convexity constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a60dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_policy_least_square(x, t):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57e551",
   "metadata": {},
   "source": [
    "**Q4**. Run the gradient descent with the obtained policy, and plot in semilogy scale the evolution of the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gradient_descent(grad_least_square, lr_policy_least_square, np.zeros(p), 1000, return_all=True, callback=least_square)\n",
    "fig, ax = plt.subplots()\n",
    "sol = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "ax.set_xlabel('iterate $t$')\n",
    "ax.set_ylabel('$\\log f(x^{(t)})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1cb82",
   "metadata": {},
   "source": [
    "**Q5**. Perform the same experiment with `A` of size 100 x 100. Does the numerics confirm the theory studied in the lecture? If not, how to explain this behavior? If yes, explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f184e",
   "metadata": {},
   "source": [
    "## Digits classification\n",
    "\n",
    "We now turn our attention to a classification problem.\n",
    "For the sake of simplicity, we will use the digits UCI ML dataset contained in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset, note that the first time you run this, it will download the dataset (could take a while)\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3acb2",
   "metadata": {},
   "source": [
    "This dataset is composed of 1797 samples, each of theme of size $8 \\times 8 = 64$ in gray level (coded on [0,255]).\n",
    "This is a low resolution dataset that will be easy to treat as a \"batch\".\n",
    "We can access the data as vectors in `digits.data` and as images in `digits.images`.\n",
    "The labels are contained in `digits.target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837587a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "(n_samples, n_features) = digits.data.shape\n",
    "print(f\"Dataset dimensions: {(n_samples, n_features)}\")\n",
    "\n",
    "# Display a few digits\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "idx = [0, 42, 345, 826]\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(digits.images[idx[i]], cmap='gray')\n",
    "    ax.set_title(f\"Label {digits.target[idx[i]]}\")\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54150977",
   "metadata": {},
   "source": [
    "**Q1**. Using `sklearn.model_selection.train_test_split`, split the dataset into a training and a test sets.\n",
    "Center the training and the test set relative to the mean image of the training set.\n",
    "Add a column of ones to take into account the potential bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test with a 80/20 ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Center the data\n",
    "X_train = (X_train - X_train.mean(axis=0))\n",
    "X_test = (X_test - X_train.mean(axis=0))\n",
    "\n",
    "# Add a column of ones to the data to account for the bias\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a724482",
   "metadata": {},
   "source": [
    "To perform our classification task, we will turn to the multinomial logistic regression, or softmax classification.\n",
    "The objective function here will be seek a weight matrix $W \\in \\mathbb{R}^{K \\times p}$\n",
    "\n",
    "$$ f(W) = \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat p(y_i = k | X_i)) + \\lambda \\| W \\|_2^2, $$\n",
    "where\n",
    "$$ \\hat p(y_i = k | X_i) = \\frac{\\exp(X_i W_k)}{\\sum_{l=0}^{K-1}\\exp(X_i W_l)} . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570c3e1",
   "metadata": {},
   "source": [
    "**Numerical stability of the softmax**. When computing the softmax of a vector, one has to be careful of the potential numerical errors due to the large values of $\\exp$ when the argument is above 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([433, 23, 766])\n",
    "proba = np.exp(scores) / np.sum(np.exp(scores))\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87db96e",
   "metadata": {},
   "source": [
    "Clearly here, we would expect to obtain `[0., 0., 1.]`.\n",
    "Remark that for any $c > 0$,\n",
    "$$\n",
    "\\sigma(z)_i\n",
    "=\n",
    "\\frac{\\exp(z_i)}{\\sum_{i=1}^p \\exp{(z_j)}}\n",
    "=\n",
    "\\frac{c\\exp(z_i)}{c\\sum_{i=1}^p \\exp{(z_j)}}\n",
    "=\n",
    "\\frac{\\exp(z_i + \\log c)}{\\sum_{i=1}^p \\exp{(z_j + \\log c)}}\n",
    "$$\n",
    "A typical choice of $c$ is given by the solution of $\\log c = - \\max_j z_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61455c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([433, 23, 766])\n",
    "scores -= np.max(scores)\n",
    "proba = np.exp(scores) / np.sum(np.exp(scores))\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3b932",
   "metadata": {},
   "source": [
    "**Q2**. Implement the `softmax_loss` function below. Take care to the potential numerical errors thanks to the trick above. _Hint_: remember that `numpy.arange(n)` build a ndarray containing all integer from 0 to `n` (included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function\n",
    "\n",
    "    Inputs:\n",
    "    - W: ndarray (n_features, K) containing weights.\n",
    "    - X: ndarray (n, n_features) containing data.\n",
    "    - y: ndarray (n,) containing training labels\n",
    "    - reg: (float) regularization\n",
    "\n",
    "    Returns the softmax loss (with regularization)\n",
    "    \"\"\"\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35171b04",
   "metadata": {},
   "source": [
    "To debug it, run the following cell. You should obtain a value close to $-\\log(1/10)$.\n",
    "\n",
    "**Q3**: Why should we obtain something close to this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random softmax weight matrix\n",
    "W = np.random.randn(64+1, 10) * 0.001\n",
    "# Compute the softmax loss\n",
    "loss = softmax_loss(W, X_train, y_train, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something more or less close to -log(0.1).\n",
    "print(f\"Loss: {loss}, expected: {-np.log(0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a284fc",
   "metadata": {},
   "source": [
    "**Q4**. Compute the gradient of the loss $\\nabla f(W)$ with respect to weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef51224",
   "metadata": {},
   "source": [
    "**Q5**. Implement the gradient of the softmax loss as a `softmax_grad` function below. _Hint_: This function will be _very_ close to `softmax_loss`. Note that it would be more adequate to write a unique function returning _both_ the loss and the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c97e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(W, X, y, reg):\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e72f8",
   "metadata": {},
   "source": [
    "**Q6**. Run the gradient descent on the loss function defined by the training set. You should use a constant step size policy. _Hint_: define `softmax_grad_train` and `softmax_loss_train` taking only `W` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.001\n",
    "\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(res['callbacks'])\n",
    "ax.set_xlabel('iterate $t$')\n",
    "ax.set_ylabel('$f(x^{(t)})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17a770",
   "metadata": {},
   "source": [
    "**Q7**. Display the training and test accuracies. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c787e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb7b3f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
